{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5f027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fd0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('./datasets/combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b81979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_index = combined_df.columns.drop(['Id','Label_test','Label_train','SalePrice'])\n",
    "new_train_data = combined_df.loc[combined_df['Label_train']==1].drop(columns=['Id','Label_test','Label_train'])\n",
    "new_test_data = combined_df.loc[combined_df['Label_test']==1].drop(columns=['Id','Label_test','Label_train'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_train_data.drop('SalePrice', axis=1),new_train_data['SalePrice'], random_state=42)\n",
    "y_train = np.log1p(y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a503a",
   "metadata": {},
   "source": [
    "# fit OLS, Ridge, Lasso\n",
    "Naively fitting all 3 linear models optimising for R^2 score. then observe which would be the best. Ridge appears to be the best. CV=100 is arbitrarily chosen as a high number of folds but without compromising on shorter run times  \n",
    "- R^2 improves for test with lasso implying that this regularisation is doing a good job at reducing bias especially compared to ridge.\n",
    "\n",
    "'*' Elasticnet is tried and it tended to use lasso. this is interesting because even though ElasticNet chooses the `l1_ratio` with the best score RidgeCV still has a better score than LassoCV. ElasticNet is dropped from further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddc8a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ols train R^2:\t\t 0.8687\n",
      "ols test R^2:\t\t 0.8892\n"
     ]
    }
   ],
   "source": [
    "ols=LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "ols_scores = cross_val_score(ols, X_train, y_train, cv=100, n_jobs=-1,scoring='r2')\n",
    "print(f'ols train R^2:\\t\\t {ols_scores.mean():.4f}')\n",
    "\n",
    "ols_test_score = ols.score(X_test,np.log1p(y_test))\n",
    "print(f'ols test R^2:\\t\\t {ols_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7263425a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge train R^2:\t 0.9421\n",
      "Ridge test R^2:\t\t 0.8911\n"
     ]
    }
   ],
   "source": [
    "ridge_cv=RidgeCV(cv=100,alphas=[0.125, 1.25, 12.5])\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "ridge_cv_score = ridge_cv.score(X_train, y_train)\n",
    "print(f'Ridge train R^2:\\t {ridge_cv_score:.4f}')\n",
    "\n",
    "ridge_cv_test_score = ridge_cv.score(X_test,np.log1p(y_test))\n",
    "print(f'Ridge test R^2:\\t\\t {ridge_cv_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e5e1bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso train R^2:\t 0.8757\n",
      "lasso test R^2:\t\t 0.8674\n"
     ]
    }
   ],
   "source": [
    "lasso_cv=LassoCV(cv=100)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "lasso_cv_score = lasso_cv.score(X_train, y_train)\n",
    "print(f'lasso train R^2:\\t {lasso_cv_score:.4f}')\n",
    "\n",
    "lasso_cv_test_score = lasso_cv.score(X_test,np.log1p(y_test))\n",
    "print(f'lasso test R^2:\\t\\t {lasso_cv_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527c7b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ols 2.37%\n",
      "ridge -5.42%\n",
      "lasso -0.95%\n"
     ]
    }
   ],
   "source": [
    "print(f'ols {(ols_test_score-ols_scores.mean())/ols_scores.mean()*100:.2f}%')\n",
    "print(f'ridge {(ridge_cv_test_score-ridge_cv_score)/ridge_cv_score*100:.2f}%')\n",
    "print(f'lasso {(lasso_cv_test_score-lasso_cv_score)/lasso_cv_score*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78ef7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elastic train R^2:\t 0.8757\n",
      "elastic test R^2:\t 0.8674\n"
     ]
    }
   ],
   "source": [
    "elastic_cv = ElasticNetCV(cv=100,l1_ratio =[0.01, .1, .3, .5, .7, .9, 1])\n",
    "elastic_cv.fit(X_train, y_train)\n",
    "elastic_cv_score = elastic_cv.score(X_train, y_train)\n",
    "print(f'elastic train R^2:\\t {elastic_cv_score:.4f}')\n",
    "\n",
    "elastic_cv_test_score = elastic_cv.score(X_test,np.log1p(y_test))\n",
    "print(f'elastic test R^2:\\t {elastic_cv_test_score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc2306c",
   "metadata": {},
   "source": [
    "## RidgeCV\n",
    "to fit Ridge, an optimal alpha is required. this is determined with Ridge CV and varying alpha values. additional step of using MSE or 'neg_mean_squared_error' is taken because the competition uses R-MSE.  \n",
    "For-loop is used to sift an optimal alpha value = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a36486",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv = RidgeCV()\n",
    "alphas=[[.01,.1,1,10,100],[.1,.5,1,2,5],[.5,.7,1,1.5,2],[1,1.25,1.5,1.75,2],[1,1.1,1.25,1.3,1.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32c680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "alpha\t1.0\n",
      "R^2\t0.9429\n",
      "tr MSE  0.00947\n",
      "========================\n",
      "alpha\t1.0\n",
      "R^2\t0.9429\n",
      "tr MSE  0.00947\n",
      "========================\n",
      "alpha\t1.5\n",
      "R^2\t0.9414\n",
      "tr MSE  0.00971\n",
      "========================\n",
      "alpha\t1.25\n",
      "R^2\t0.9421\n",
      "tr MSE  0.00959\n",
      "========================\n",
      "alpha\t1.25\n",
      "R^2\t0.9421\n",
      "tr MSE  0.00959\n"
     ]
    }
   ],
   "source": [
    "for a in alphas:\n",
    "    ridge_cv = RidgeCV(alphas=a,scoring='neg_mean_squared_error')\n",
    "    ridge_cv.fit(X_train, y_train)\n",
    "    print('='*24)\n",
    "    print(f'alpha\\t{ridge_cv.alpha_}\\nR^2\\t{ridge_cv.score(X_train, y_train):.4f}')\n",
    "    y_pred = ridge_cv.predict(X_train)\n",
    "    ridge_cv_mse = mean_squared_error(y_train, y_pred)\n",
    "    print(f'tr MSE  {ridge_cv_mse:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bd921f",
   "metadata": {},
   "source": [
    "# Comparing MSE\n",
    "observing MSE, OLS is the least in both train and test set. This may imply that the overfitted OLS may be useful in modelling the multi-variable aspects of house buying.\n",
    "\n",
    "OLS is simple and we use CV to determine the expected OLS scores. additional step of computing MSE or 'neg_mean_squared_error' is taken because the competition uses R-MSE.\n",
    "\n",
    "'*' we pay special atttention that y is log transformed and compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d706c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_scores = cross_val_score(ols, X_train, y_train, cv=100, n_jobs=-1,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c5ba0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse\t\t18,705.268739991196\n",
      "test rmse\t\t21,371.13367972099\n",
      "delta of train: \t14.25%\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.expm1(ols.predict(X_train))\n",
    "ols_mse = mean_squared_error(np.expm1(y_train), y_pred)\n",
    "train_mse = ols_mse**.5\n",
    "print(f'train rmse\\t\\t{ols_mse**.5:,}')\n",
    "\n",
    "y_pred = np.expm1(ols.predict(X_test))\n",
    "ols_mse = mean_squared_error(y_test, y_pred)\n",
    "test_mse = ols_mse**.5\n",
    "print(f'test rmse\\t\\t{ols_mse**.5:,}')\n",
    "\n",
    "print(f'delta of train: \\t{(test_mse-train_mse)/train_mse*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0566b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse\t\t19,606.82807081815\n",
      "test rmse\t\t21,189.585405414226\n",
      "delta of train: \t8.07%\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.expm1(ridge_cv.predict(X_train))\n",
    "ridge_cv_mse = mean_squared_error(np.expm1(y_train), y_pred)\n",
    "train_mse = ridge_cv_mse**.5\n",
    "print(f'train rmse\\t\\t{ridge_cv_mse**.5:,}')\n",
    "\n",
    "y_pred = np.expm1(ridge_cv.predict(X_test))\n",
    "ridge_cv_mse = mean_squared_error(y_test, y_pred)\n",
    "test_mse = ridge_cv_mse**.5\n",
    "print(f'test rmse\\t\\t{ridge_cv_mse**.5:,}')\n",
    "\n",
    "print(f'delta of train: \\t{(test_mse-train_mse)/train_mse*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b1ddea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse\t\t30,235.400253568747\n",
      "test rmse\t\t26,322.084295295113\n",
      "delta of train: \t-12.94%\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.expm1(lasso_cv.predict(X_train))\n",
    "lasso_cv_mse = mean_squared_error(np.expm1(y_train), y_pred)\n",
    "train_mse = lasso_cv_mse**.5\n",
    "print(f'train rmse\\t\\t{lasso_cv_mse**.5:,}')\n",
    "\n",
    "y_pred = np.expm1(lasso_cv.predict(X_test))\n",
    "lasso_cv_mse = mean_squared_error(y_test, y_pred)\n",
    "test_mse = lasso_cv_mse**.5\n",
    "print(f'test rmse\\t\\t{lasso_cv_mse**.5:,}')\n",
    "\n",
    "print(f'delta of train: \\t{(test_mse-train_mse)/train_mse*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91420424",
   "metadata": {},
   "source": [
    "## LassoCV\n",
    "LassoCV has a robust determination method of determining optimal alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a669f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
